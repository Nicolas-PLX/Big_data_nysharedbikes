# -*- coding: utf-8 -*-
"""NYSharedBikes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Px4rVvgcS4UCsKhLsFzoiPXGyj14xrF0

# NY Shared Bikes

### PREAMBULE

Voici notre rendu concernant l'analyse de la base de données annuelles des , fait par Hélène HU et Nicolas PENELOUX. Il y aura deux rendus, un avec compilation, et un sans, dans le cas ou vous auriez des problèmes à compiler le code du fichier : nous avions nous même eu quelques problèmes à ce sujet, notamment pour pyspark que nous utilisons ; nous voulons vous montrez que nous avons bien travailler le devoir.

Pour le gestionnaire de données, nous avons utilisé pyspark. Notre choix était motivé parce qu'il nous semblait plus pratique de l'utiliser que pandas, même si dans certains cas il semblerait que pandas soit plus efficace, notamment pour l'utilisation de plotly : parfois, plotly ne fonctionnait pas avec des dataframes spark, et dans ce cas nous utilisons les dataframes pandas, en transformant les dfs spark avec "**toPandas()**".

Egalement, nous avons privilégié plotly à altair, car les objets graphiques produit par plotly nous semblait plus agréable à regarder que ceux d'altair.

Enfin, nous avons laissé les lignes de commandes pour installer pyspark, ainsi qu'une autre pour installer une extension particulière de plotly (que nous utilisons à la question 5), dans le cas où vous ne l'aviez pas. En espérant que cela ne gâche pas votre compilation.
"""

!pip install pyspark
!pip install plotly-calplot
!pip install geojson geopandas plotly geopy
!pip install ipyleaflet

"""### Téléchargement des fichiers

On initialise une session Spark nommée "NYSharedBikes". Ensuite, on génère une liste de liens de téléchargement pour les données de Citibike couvrant la période de 2014 à 2023. Pour chaque lien de téléchargement, on télécharge le fichier zip correspondant et on extrait son contenu dans le dossier de sortie. Chaque fichier CSV extrait est ensuite traité, à condition que son nom ne contienne pas '/.', afin d'exclure les fichiers cachés. On détermine le chemin du fichier CSV et on extrait l'année et le mois de son nom. Le fichier CSV est ensuite chargé dans un DataFrame Spark, en interprétant toutes les colonnes comme des chaînes de caractères pour éviter des problèmes d'interprétation des dates. On ajoute les DataFrames à un dictionnaire, où chaque clé correspond à une combinaison d'année et de mois, et chaque valeur est le DataFrame correspondant. .
"""

import re
import requests
import zipfile
import io
import os
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pyspark as ps
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import *
from pyspark.sql import functions as F
from pyspark.storagelevel import StorageLevel
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer
import plotly.graph_objects as go



spark = SparkSession.builder \
    .appName("NYSharedBikes") \
    .master("local[*]") \
    .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
    .getOrCreate()

links = ["https://s3.amazonaws.com/tripdata/20{}-citibike-tripdata.zip".format(i) for i in range(14, 24)]

dataframes = {}

output_folder = "citibike_data"

# Create the output_folder if it doesn't exist
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

for link in links:
    filename = link.split("/")[-1]
    response = requests.get(link)
    with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zip_ref:
        zip_ref.extractall(output_folder)
        for file in zip_ref.namelist():
            # Exclude hidden files
            if file.endswith('.csv') and '/.' not in file:
              csv_path = os.path.join(output_folder, file)

              # Extract the month and year from the filename
              parts = os.path.basename(csv_path).split('_')[0]
              year_month = parts[:6]  # "201410" for example

              # We do not use infer schema here because there is an issue where Spark interprets some timestamps as dates.
              # Therefore, we decide to interpret all the attributes as strings and change their types manually later in our code.
              df = spark.read.format("csv").option("header", "true").option("sep", ",").load(csv_path)

              # Add the DataFrame to the dictionary, grouped by year and month
              if year_month in dataframes:
                  dataframes[year_month] = dataframes[year_month].union(df)
              else:
                  dataframes[year_month] = df

    print("Download of file " + filename + " completed.")

"""### Nettoyage des données

Dans le bloc de code suivant, on fournit plusieurs fonctions pour le traitement des données d'un ensemble de trajets de vélos. La fonction `rename_columns` renomme les colonnes du DataFrame selon un mapping prédéfini. Ensuite, la fonction `format_timestamp` convertit les colonnes de type timestamp dans un format spécifié. La fonction `cast_columns` convertit les données des colonnes spécifiées en types de données appropriés. La fonction `add_missing_columns` ajoute les colonnes manquantes nécessaires au DataFrame, en les initialisant avec des valeurs nulles. La fonction `clean_df` applique toutes les transformations précédentes au DataFrame en utilisant un ordre spécifique pour les colonnes et retourne le DataFrame nettoyé. Enfin, la fonction `string_indexer` prend un DataFrame en entrée et applique la transformation StringIndexer à trois colonnes spécifiques (rideable_type, member_casual, user_type), en assignant les valeurs nulles à une catégorie spéciale.
"""

def rename_columns(df):
    column_mapping = {
        'tripduration': 'trip_duration',
        'starttime': 'started_at',
        'stoptime': 'ended_at',
        'start station id': 'start_station_id',
        'start station name': 'start_station_name',
        'start station latitude': 'start_lat',
        'start station longitude': 'start_lng',
        'end station id': 'end_station_id',
        'end station name': 'end_station_name',
        'end station latitude': 'end_lat',
        'end station longitude': 'end_lng',
        'bikeid': 'bike_id',
        'usertype': 'user_type',
        'birth year': 'birth_year',
        'Trip Duration': 'trip_duration',
        'Start Time': 'started_at',
        'Stop Time': 'ended_at',
        'Start Station ID': 'start_station_id',
        'Start Station Name': 'start_station_name',
        'Start Station Latitude': 'start_lat',
        'Start Station Longitude': 'start_lng',
        'End Station ID': 'end_station_id',
        'End Station Name': 'end_station_name',
        'End Station Latitude': 'end_lat',
        'End Station Longitude': 'end_lng',
        'Bike ID': 'bike_id',
        'User Type': 'user_type',
        'Birth Year': 'birth_year'
    }
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns:
            df = df.withColumnRenamed(old_name, new_name)
    return df


def format_timestamp(df):
    if 'started_at' in df.columns:
      df = df.withColumn('started_at', coalesce(
        to_timestamp(df['started_at'], 'MM/d/yyyy HH:mm:ss'),
        to_timestamp(df['started_at'], 'yyyy-MM-dd HH:mm:ss'),
        to_timestamp(df['started_at'], 'M/d/yyyy H:mm')))
    if 'ended_at' in df.columns:
      df = df.withColumn('ended_at', coalesce(
        to_timestamp(df['ended_at'], 'MM/d/yyyy HH:mm:ss'),
        to_timestamp(df['started_at'], 'yyyy-MM-dd HH:mm:ss'),
        to_timestamp(df['ended_at'], 'M/d/yyyy H:mm')))

    return df

def cast_columns(df):
    columns_to_cast = {
        'trip_duration': 'int',
        'start_lat': 'double',
        'start_lng': 'double',
        'end_lat': 'double',
        'end_lng': 'double',
        'bike_id': 'int',
        'gender': 'int',
        'birth_year': 'int'
    }

    for column, data_type in columns_to_cast.items():
        if column in df.columns:
            df = df.withColumn(column, col(column).cast(data_type))

    return df

def add_missing_columns(df):
    required_columns = {
        'trip_duration': IntegerType(),
        'bike_id': StringType(),
        'user_type': StringType(),
        'birth_year': IntegerType(),
        'ride_id': StringType(),
        'rideable_type': StringType(),
        'member_casual': StringType(),
        'gender': IntegerType()
    }
    for column, column_type in required_columns.items():
        if column not in df.columns:
            df = df.withColumn(column, lit(None).cast(column_type))
    return df

def string_indexer(df):
    indexer1 = StringIndexer(inputCol="rideable_type", outputCol="rideable_type_indexed")
    indexer2 = StringIndexer(inputCol="member_casual", outputCol="member_casual_indexed")
    indexer3 = StringIndexer(inputCol="user_type", outputCol="user_type_indexed")

    df = indexer1.setHandleInvalid("keep").fit(df).transform(df)
    df = indexer2.setHandleInvalid("keep").fit(df).transform(df)
    df = indexer3.setHandleInvalid("keep").fit(df).transform(df)

    return df

def clean_df(df):
    df = rename_columns(df)
    df = format_timestamp(df)
    df = cast_columns(df)
    df = add_missing_columns(df)
    df = string_indexer(df)
    desired_order =["ride_id", "started_at", "ended_at", "start_station_name", "start_station_id", "end_station_name", "end_station_id", "start_lat", "start_lng", "end_lat", "end_lng", "trip_duration", "rideable_type", "rideable_type_indexed", "bike_id", "member_casual", "member_casual_indexed", "user_type", "user_type_indexed", "birth_year", "gender" ]
    df = df.select(desired_order)

    return df

"""### Star schema

La fonction star_schema transforme le DataFrame pour créer un schéma en étoile. Elle sélectionne les données des stations de départ et d'arrivée, puis les combine en un ensemble distinct. Elle extrait également les informations essentielles sur les trajets. En retour, la fonction produit deux DataFrames distincts : l'un pour les stations et l'autre pour les trajets.

Dans le second bloc de code, on commence par faire une copie des DataFrames originaux, puis initialise deux dictionnaires vides pour stocker les stations et les trajets. Ensuite, pour chaque année et mois dans les données, les DataFrames sont nettoyés à l'aide de la fonction clean_df, puis mis en cache avec la méthode persist. En parallèle, la fonction star_schema est utilisée pour extraire les informations sur les stations et les trajets de chaque DataFrame nettoyé. On choisit au hasard le mois de janvier 2019 pour afficher un aperçu des trois premières lignes de chaque DataFrame, le nombre de partitions pour le DataFrame original, aisni qu'un aperçu des trois premières lignes des DataFrames de stations et de trajets.

Le fait de mettre en cache et de traiter simultanément les opérations de nettoyage (clean_df) et de structuration (star schema) dans une seule boucle for permet de réduire le temps d'exécution.
"""

def star_schema(df):
    stations = df.select(
        col('start_station_id').alias('station_id'),
        col('start_station_name').alias('station_name'),
        col('start_lat').alias('lat'),
        col('start_lng').alias('lng')
    ).union(
        df.select(
            col('end_station_id').alias('station_id'),
            col('end_station_name').alias('station_name'),
            col('end_lat').alias('lat'),
            col('end_lng').alias('lng')
        )
    ).distinct()

    trips = df.select('start_station_id', 'end_station_id', 'started_at', 'ended_at' )

    return stations, trips

dataframescopy = dataframes.copy()
stations = {}
trips = {}
for year_month, df in dataframescopy.items():
    # Cleaning
    dataframescopy[year_month] = clean_df(df)

    # Here we decide to persist our dataframe in cache.
    dataframescopy[year_month] = dataframescopy[year_month].persist()

    # Star schema
    stations[year_month], trips[year_month] = star_schema(dataframescopy[year_month])
    print("Cleaning "+ year_month + " done")
    # Display a particular case
    if '201901' in year_month:
      print(dataframescopy[year_month])
      dataframescopy[year_month].show(3)
      print('Number of partitions: ' + str(df.rdd.getNumPartitions()))
      stations[year_month].show(3)
      trips[year_month].show(3)

"""### Partitionnement format parquet

Pour le partitionnement format parquet, on décide de partitionner par année et par mois. C'est le partitionnement le plus logique que nous ayons trouvé, car il est probable que nous recherchons des informations sur les dates. Le partitionnement est un processus très long, assurer vous d'avoir le temps ou sauter le pour le faire plus tard.
"""

for year_month, df in dataframescopy.items():
  df_parquet = dataframescopy[year_month].withColumn("year", year(col("started_at"))).withColumn("month", month(col("started_at")))
  df_parquet.write.mode("append").partitionBy("year", "month").parquet("parquet_data/citibike-cleaned.parquet")

ls -al  parquet_data/citibike-cleaned.parquet/*/

"""### Visualisation des données avant et après le covid (2020)

Dans le bloc de code qui suit, on met en œuvre plusieurs fonctions pour analyser les données des trajets de vélos partagés. Tout d'abord, on utilise la formule Haversine pour calculer la distance entre deux points sur la surface de la Terre. Ensuite, on définit des fonctions pour analyser les données en fonction du jour de la semaine, du nombre de paires de lieux de prise en charge/dépose, du genre, de la tranche d'âge, du type de vélo et de la distance parcourue. Chaque fonction traite les DataFrames en appliquant des transformations spécifiques, puis génère des visualisations à l'aide de la bibliothèque Plotly. On sélectionne ensuite un mois de 2019 et un mois de 2022 au hasard pour comparer les données et réduire le temps d'exécution. Enfin, on parcourt tous les mois disponibles en 2019 et 2022 pour effectuer les mêmes analyses et générer des visualisations comparatives. Cette dernière partie peut prendre du temps à exécuter, vous pouvez la sauter si vous le souhaitez.
"""

# Haversine formula to calculate distance between two points on the Earth's surface
def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0  # Radius of Earth in kilometers
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

# Register the Haversine function as a UDF
haversine_udf = udf(haversine, DoubleType())

def distance_day_of_week(df1, df2, t1, t2):
    def process_df(df):
        df = df.withColumn("trip_distance", haversine(col("start_lat"), col("start_lng"), col("end_lat"), col("end_lng")))
        df = df.withColumn("day_of_week", dayofweek("started_at"))
        agg_df = df.groupBy("day_of_week").agg({"trip_distance": "sum"})
        return agg_df.toPandas()

    day_of_week_mapping = {
        1: 'Sunday',
        2: 'Monday',
        3: 'Tueday',
        4: 'Wednesday',
        5: 'Thursday',
        6: 'Friday',
        7: 'Saturday'
    }

    pandas_df1 = process_df(df1)
    pandas_df2 = process_df(df2)

    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    fig.add_trace(go.Bar(x=pandas_df1['day_of_week'].replace(day_of_week_mapping), y=pandas_df1['sum(trip_distance)'], name='2019'), row=1, col=1)
    fig.add_trace(go.Bar(x=pandas_df2['day_of_week'].replace(day_of_week_mapping), y=pandas_df2['sum(trip_distance)'], name='2022'), row=1, col=2)

    fig.update_layout(title_text="Trip Distance Distribution by Day of Week")
    fig.show()

def count_location_pair(df1, df2, t1, t2):
    def process_df(df):
        df = df.withColumn("location_pair", concat(col("start_station_name"), lit(" ----> "), col("end_station_name")))
        trip_count_by_location = df.groupBy("location_pair").count().orderBy("count", ascending=False)
        return trip_count_by_location.toPandas()

    pandas_df1 = process_df(df1)
    pandas_df2 = process_df(df2)

    fig = make_subplots(rows=1, cols=2, specs=[[{"type": "table"}, {"type": "table"}]], subplot_titles=(t1, t2))

    fig.add_trace(
        go.Table(
            header=dict(values=["Location Pair", "Trip Count"]),
            cells=dict(values=[pandas_df1['location_pair'], pandas_df1['count']])
        ), row=1, col=1,
    )
    fig.add_trace(
        go.Table(
            header=dict(values=["Location Pair", "Trip Count"]),
            cells=dict(values=[pandas_df2['location_pair'], pandas_df2['count']])
        ), row=1, col=2,
    )
    fig.update_layout(title="The Number of Trips for Each Pickup/Dropoff Location Pair")
    fig.show()

def distance_gender(df1, df2, t1, t2):
    def process_df(df):
        df = df.withColumn("trip_distance", haversine(col("start_lat"), col("start_lng"), col("end_lat"), col("end_lng")))
        trip_distance_by_gender = df.groupBy("gender").agg(sum("trip_distance").alias("sum_trip_distance"))
        return trip_distance_by_gender.toPandas()

    gender_mapping = {
        0: "Unknown",
        1: "Male",
        2: "Female"
    }

    pandas_df1 = process_df(df1)
    pandas_df2 = process_df(df2)

    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    fig.add_trace(go.Bar(x=pandas_df1['gender'].replace(gender_mapping), y=pandas_df1['sum_trip_distance'], name='2019'), row=1, col=1)
    fig.add_trace(go.Bar(x=pandas_df2['gender'].replace(gender_mapping), y=pandas_df2['sum_trip_distance'], name='2022'), row=1, col=2)

    fig.update_layout(title="Total Trip Distance by Gender")
    fig.show()

def categorize_age(birth_year):
    age_ranges = {
        "15-24": (15, 24),
        "25-44": (25, 44),
        "45-54": (45, 54),
        "55-64": (55, 64),
        "65+": (65, float('inf'))
    }
    current_year = datetime.now().year
    age = current_year - birth_year
    for range_name, (min_age, max_age) in age_ranges.items():
        if min_age <= age <= max_age:
            return range_name
    return "Unknown"

categorize_age_udf = udf(categorize_age, StringType())

def distance_age_range(df1, df2, t1, t2):
    def process_df(df):
        df = df.filter(col("birth_year").isNotNull())
        df = df.withColumn("age_range", categorize_age_udf(col("birth_year")))
        df = df.withColumn("trip_distance", haversine(col("start_lat"), col("start_lng"), col("end_lat"), col("end_lng")))
        agg_df = df.groupBy("age_range").agg({"trip_distance": "sum"})
        return agg_df.toPandas()

    pandas_df1 = process_df(df1)
    pandas_df2 = process_df(df2)

    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    fig.add_trace(go.Bar(x=pandas_df1['age_range'], y=pandas_df1['sum(trip_distance)'], name='2019'), row=1, col=1)
    fig.add_trace(go.Bar(x=pandas_df2['age_range'], y=pandas_df2['sum(trip_distance)'], name='2022'), row=1, col=2)

    fig.update_layout(title="Trip distance distribution by age group")
    fig.show()

def distance_bike(df1, df2, t1, t2):
    def process_df(df):
        df = df.withColumn('trip_distance', haversine(col("start_lat"), col("start_lng"), col("end_lat"), col("end_lng")))
        agg_df = df.groupBy("rideable_type").agg({"trip_distance": "sum"})
        return agg_df.toPandas()

    pandas_df1 = process_df(df1)
    pandas_df2 = process_df(df2)

    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    fig.add_trace(go.Bar(x=pandas_df1['rideable_type'], y=pandas_df1['sum(trip_distance)'], name='2019'), row=1, col=1)
    fig.add_trace(go.Bar(x=pandas_df2['rideable_type'], y=pandas_df2['sum(trip_distance)'], name='2022'), row=1, col=2)

    fig.update_layout(title="Trip distance distribution by bike type")
    fig.show()

# We decided to select one month in 2019 and 2022 to compare for reducing the execution time.
# If you want to visualize all months in 2019 and 2023, you can execute the next block of code, but it will take a long time.

df_2019 = dataframescopy['201903']
df_2022 = dataframescopy['202203']
t1, t2 = '2019-03', '2022-03'
distance_day_of_week(df_2019, df_2022, t1, t2)
count_location_pair(df_2019, df_2022, t1, t2)
distance_gender(df_2019, df_2022, t1, t2)
distance_age_range(df_2019, df_2022, t1, t2)
distance_bike(df_2019, df_2022, t1, t2)

df_2019 = {key: df for key, df in dataframescopy.items() if '2019' in key}
df_2022 = {key: df for key, df in dataframescopy.items() if '2022' in key}

for (t1, df1), (t2, df2) in zip(df_2019.items(), df_2022.items()):
    distance_day_of_week(df1, df2, t1, t2)
    count_location_pair(df1, df2, t1, t2)
    distance_gender(df1, df2, t1, t2)
    distance_age_range(df1, df2, t1, t2)
    distance_bike(df1, df2, t1, t2)

"""

#### Assessing seasonalities and looking at time series

Pour cette partie, on travaillera aussi pour les mois de mars 2019 et 2022, comme la partie précédente, et si vous le souhaitez, vous pourrez calculer pour chaque mois de 2019 et 2022, mais la compilation est longue.  

On se retrouve ici avec huit fonctions, quatre de calcul et quatre d'affichage.
Les quatre de calcul sont là pour nous donner un dataframe réduit, correspondant à la recherche que nous voulons visualiser :  


*   `calculate_pickups_docks` calcule le dataframe correspondant au nombre de dépôt / récupération sur nos stations : On le fait en calculant le nombre d'élément dans le dataframe correspondant à l'heure pour un jour de la semaine.
*   `calculate_average_distance` calcule le dataframe correspondant à la distance moyenne des trajets entre station par heure de la journée pour les jours de la semaine. De même façon, on récupère le jour de la semaine et l'heure, et on calcule la distance en utilisant la fonction `haversine` précédente. On calcule enfin la moyenne par heure et par jour de la semaine.
* `calculate_average_trip_duration` calcule le dataframe correspondant à la durée moyenne d'un trajet.
* enfin `calculate_average_ongoing_trips` calcule le dataframe moyen des trajets en cours. Pour se faire, on créé une séquence d'heures entre l'heure de début et l'heure de fin pour chaque trajet, on récupère l'heure puis on compte les trajets qui sont en cours pour chaque heure et chaque jour de la semaine.

Les quatres fonction d'affichage sont sensiblement les même : on créé sous forme de heatmap, des graphiques qui vont afficher pour chaque jour de la semaine, chaque heure le nombre que nous avons calculé précédemment."""

# We decided to select one month in 2019 and 2022 to compare for reducing the execution time.
df_2019 = dataframescopy['201903']
df_2022 = dataframescopy['202203']

def calculate_pickups_docks(df):
  # On récupère le jour de la semaine et les heures de la journée
    df = df.withColumn("day_of_week", dayofweek("started_at"))
    df = df.withColumn("hour_of_day", hour("started_at"))
    # On compte
    pickups_docks = df.groupBy("day_of_week", "hour_of_day").count()
    return pickups_docks

pickups_docks_2019 = calculate_pickups_docks(df_2019)
pickups_docks_2022 = calculate_pickups_docks(df_2022)



def calculate_average_distance(df):
  # On récupère les jours de la semaine, les heures de la journée et on calcule la distance
    df = df.withColumn("day_of_week", dayofweek("started_at"))
    df = df.withColumn("hour_of_day", hour("started_at"))
    df = df.withColumn("trip_distance", haversine(col("start_lat"), col("start_lng"), col("end_lat"), col("end_lng")))
    avg_distance = df.groupBy("day_of_week", "hour_of_day").agg({"trip_distance": "avg"})
    return avg_distance

avg_distance_2019 = calculate_average_distance(df_2019)
avg_distance_2022 = calculate_average_distance(df_2022)


def calculate_average_trip_duration(df):
  # On calcule la durée moyenne d'un voyage
    df = df.withColumn("day_of_week", dayofweek("started_at"))
    df = df.withColumn("hour_of_day", hour("started_at"))
    avg_trip_duration = df.groupBy("day_of_week", "hour_of_day").agg({"trip_duration": "avg"})
    return avg_trip_duration

avg_trip_duration_2019 = calculate_average_trip_duration(df_2019)
avg_trip_duration_2022 = calculate_average_trip_duration(df_2022)


def calculate_average_ongoing_trips(df):
    # On Ajoute des colonnes pour l'heure de début et l'heure de fin de chaque trajet
    df = df.withColumn("hour_start", hour("started_at"))
    df = df.withColumn("hour_end", hour("ended_at"))

    # On créé une séquence d'heures entre l'heure de début et l'heure de fin pour chaque trajet
    df = df.withColumn("hours", explode(sequence(col("started_at"), col("ended_at"), expr("INTERVAL 1 HOUR"))))

    # On extrait l'heure et le jour de la semaine de la colonne "hours"
    df = df.withColumn("hour_of_day", hour(col("hours")))
    df = df.withColumn("day_of_week", dayofweek(col("hours")))

    # On compte les trajets en cours pour chaque heure et chaque jour de la semaine
    ongoing_trips = df.groupBy("day_of_week", "hour_of_day", "hours").count()

    # Enfin, on calcule la moyenne des trajets en cours pour chaque heure et chaque jour de la semaine
    average_ongoing_trips = ongoing_trips.groupBy("day_of_week", "hour_of_day").agg(avg("count").alias("average_ongoing_trips"))

    return average_ongoing_trips

# Calculer les trajets moyens en cours
average_ongoing_trips_2019 = calculate_average_ongoing_trips(df_2019)
average_ongoing_trips_2022 = calculate_average_ongoing_trips(df_2022)

# Mapping des jours de la semaine
day_of_week_mapping = {
    1: 'Sunday',
    2: 'Monday',
    3: 'Tueday',
    4: 'Wednesday',
    5: 'Thursday',
    6: 'Friday',
    7: 'Saturday'
}

def plot_pickups_docks(df1, df2, t1, t2):
      # Conversion des dataframes spark en pandas

    pandas_df1 = df1.toPandas()
    pandas_df2 = df2.toPandas()


    # Création de la heatmap avec Plotly pour chaque df
    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    # Conversion des données : on oublie pas de remplacer les valeurs indexé par leurs vrai valeur
    fig.add_trace(go.Heatmap(x=pandas_df1['hour_of_day'], y=pandas_df1['day_of_week'].replace(day_of_week_mapping), z=pandas_df1['count'], name=t1), row=1, col=1)
    fig.add_trace(go.Heatmap(x=pandas_df2['hour_of_day'], y=pandas_df2['day_of_week'].replace(day_of_week_mapping), z=pandas_df2['count'], name=t2), row=1, col=2)

    fig.update_layout(title="Number of Pickups/Docks by Day of Week and Hour of Day")
    fig.show()

plot_pickups_docks(pickups_docks_2019, pickups_docks_2022, '2019', '2022')

def plot_average_distance(df1, df2, t1, t2):
    # Conversion des dataframes spark en pandas
    pandas_df1 = df1.toPandas()
    pandas_df2 = df2.toPandas()


    # Création de la heatmap avec Plotly pour chaque df
    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))

    # Conversion des données : on oublie pas de remplacer les valeurs indexé par leurs vrai valeur
    fig.add_trace(go.Heatmap(x=pandas_df1['hour_of_day'], y=pandas_df1['day_of_week'].replace(day_of_week_mapping), z=pandas_df1['avg(trip_distance)'], name=t1), row=1, col=1)
    fig.add_trace(go.Heatmap(x=pandas_df2['hour_of_day'], y=pandas_df2['day_of_week'].replace(day_of_week_mapping), z=pandas_df2['avg(trip_distance)'], name=t2), row=1, col=2)

    fig.update_layout(title="Average Distance by Day of Week and Hour of Day")
    fig.show()

plot_average_distance(avg_distance_2019, avg_distance_2022, '2019', '2022')


def plot_average_trip_duration(df1, df2, t1, t2):
  # Conversion des dataframes spark en pandas
    pandas_df1 = df1.toPandas()
    pandas_df2 = df2.toPandas()

  # Création de la heatmap avec Plotly pour chaque df
    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))
  # Conversion des données : on oublie pas de remplacer les valeurs indexé par leurs vrai valeur
    fig.add_trace(go.Heatmap(x=pandas_df1['hour_of_day'], y=pandas_df1['day_of_week'].replace(day_of_week_mapping), z=pandas_df1['avg(trip_duration)'], name=t1), row=1, col=1)
    fig.add_trace(go.Heatmap(x=pandas_df2['hour_of_day'], y=pandas_df2['day_of_week'].replace(day_of_week_mapping), z=pandas_df2['avg(trip_duration)'], name=t2), row=1, col=2)

    fig.update_layout(title="Average Trip Duration by Day of Week and Hour of Day")
    fig.show()

plot_average_trip_duration(avg_trip_duration_2019, avg_trip_duration_2022, '2019', '2022')



def plot_average_ongoing_trips(df1, df2, t1, t2):
  # Conversion des dataframes spark en pandas
    pandas_df1 = df1.toPandas()
    pandas_df2 = df2.toPandas()


    # Conversion des données : on oublie pas de remplacer les valeurs indexé par leurs vrai valeur
    pandas_df1['day_of_week'] = pandas_df1['day_of_week'].replace(day_of_week_mapping)
    pandas_df2['day_of_week'] = pandas_df2['day_of_week'].replace(day_of_week_mapping)

  # Création de la heatmap avec Plotly pour chaque df
    fig = make_subplots(rows=1, cols=2, subplot_titles=(t1, t2))
    fig.add_trace(go.Heatmap(x=pandas_df1['hour_of_day'], y=pandas_df1['day_of_week'], z=pandas_df1['average_ongoing_trips'], coloraxis='coloraxis', name=t1), row=1, col=1)
    fig.add_trace(go.Heatmap(x=pandas_df2['hour_of_day'], y=pandas_df2['day_of_week'], z=pandas_df2['average_ongoing_trips'], coloraxis='coloraxis', name=t2), row=1, col=2)

    fig.update_layout(
        title="Average Ongoing Trips by Day of Week and Hour of Day",
        coloraxis={'colorscale': 'Viridis'},
        xaxis_title="Hour of Day",
        yaxis_title="Day of Week"
    )
    fig.show()



plot_average_ongoing_trips(average_ongoing_trips_2019, average_ongoing_trips_2022, 'March 2019', 'March 2022')

# Permet de calculer pour chaque mois de 2019 et 2022
df_2019 = {key: df for key, df in dataframescopy.items() if '2019' in key}
df_2022 = {key: df for key, df in dataframescopy.items() if '2022' in key}

for (t1, df1), (t2, df2) in zip(df_2019.items(), df_2022.items()):
    pickups_docks_2019 = calculate_pickups_docks(df1)
    pickups_docks_2022 = calculate_pickups_docks(df2)
    plot_pickups_docks(pickups_docks_2019, pickups_docks_2022, '2019', '2022')

    avg_distance_2019 = calculate_average_distance(df1)
    avg_distance_2022 = calculate_average_distance(df2)
    plot_average_distance(avg_distance_2019, avg_distance_2022, '2019', '2022')

    avg_trip_duration_2019 = calculate_average_trip_duration(df1)
    avg_trip_duration_2022 = calculate_average_trip_duration(df2)
    plot_average_trip_duration(avg_trip_duration_2019, avg_trip_duration_2022, '2019', '2022')

    average_ongoing_trips_2019 = calculate_average_ongoing_trips(df1)
    average_ongoing_trips_2022 = calculate_average_ongoing_trips(df2)
    plot_average_ongoing_trips(average_ongoing_trips_2019, average_ongoing_trips_2022, '2019', '2022')

"""### Monitor Job Execution

1. Use explain method.
2. Do the Analyzed Logical Plan and Optimized Logical Plan differ? Spot the differences
if any. How would a RDBMS proceed with such a query?
3. How does the physical plan differ from the Optimized Logical Plan? What are the
keywords you would not expects in a RDBMS? What is their meaning?
4. Inspect the stages on Spark UI. How many stages are necessary to complete the Spark
job? What are the roles of HashAggregate and Exchange hashpartitioning?
5. Does the physical plan perform shuffle operations? If yes how many?
6. What are tasks with respect to stages (in Spark language)? How many tasks are your
stages made of?


Réponses :

1. On utilise explain sur une requête basique du comptage de l'id des stations de départ pour la période de Mars 2019.

2. Oui les deux plans diffèrent. Le plan logique optimisé réduit les colonnes projetées uniquement à `start_station_id` ce qui est nécessaire pour l'agrégation. Cela diminue la quantité de données traitées et optimise les performances. De plus, le plan optimisé inclut une étape `InMemoryRelation`, ce qui signifie que les données sont mises en cache pour des lectures plus rapides.
Dans un RDBMS, une requête similaire serait optimisée en sélectionnant uniquement les colonnes nécessaires et en utilisant sûrement des index pour accélrer l'agrégation. Les RDBMS peuvent utilisé des plans d'éxécution similaire mais probablement pas des méthodes comme `InMemoryRelation`, parce que tant bien que même les RDBMS sont efficaces, ce genre de méthode ne semble pas être courantes chez les RDBMS traditionnels.

3. Les plans physiques diffèrent eux aussi. Les différences sont les suivantes : `HashAggregate` qui est utilisé pour l'agrégation, on hache les clés pour une meilleur distribution, `Exchange haspartitioning` qui indique une opération de "shuffle" des données pour garantir que les données sont correctement paritionnées avant l'agrégation, et `InMemoryTableScan` qui lis des données en mémoire, comme dans la question précédente qui les mettait en mémoire.
Les mots clés non courant dans un RDBMS sont les 3 méthodes précédemment cités. En effet, `HashAggregate` semble être propre à Spark, `Exchange haspartitioning` représente une opération de "shuffle" donc ("redistribution", "mélange" en français), qui est courant dans les RDBMS mais pas sous ce nom, et `InMemoryTableScan` qui consiste à lire les données mis dans le cache : si des méthodes de mise en cache sont peu courante dans les RDBMS, celle de lecture le sont tout autant.

4. D'après le plan physique, on suppose qu'il y a au moins deux stages, un pour le scan initial et un autre pour l'agrégation après l'échange de partitions. `HashAggregate` comme on l'a dit plus tôt, permet d'agréger les données par hacahge des clés, et `Exchange haspartitioning` redistribue les données en fonction de la clé d'agrégation (dans notre cas `start_station_id`)

5. Le plan physique indique bien qu'il y a une opération de "shuffle" avec `Exchange hashpartitioning(start_station_id#56497,200)`. Il y a donc au moins une opération de shuffle.

6. Un stage est une unité de travail qui comprend un ensemble de tâches effectués en parallèles qui peuvent être éxécutées sans dépendance d'un shuffle, alors que les tâches sont des unités de travail au sein d'un stage, chaque tâche exécutant le même code sur différentes parties des données.
Le nombre de tâches dépend du nombre de partitions. Dans notre cas, on peut prendre exemple sur `Exchange hashpartitioning(start_station_id#56497,200)` : cette ligne nous indique qu'il y a 200 partitions, donc 200 tâches pour ce stage.
"""

df = dataframescopy["201903"]
df = df.groupBy("start_station_id").count()
df.explain(mode='extended')

df.show()

"""### Dive into spatial information problems

Concernant la visualisation pour les espaces, on travaillera sur l'année 2019,également on s'occupera dans certains cas du mois de mars 2019, pour pouvoir vous affichez le travail plus facilement sans que cela ne prenne trop de temps, mais vous retrouverez le quasi même code pour l'année 2019.

Pour cette partie, on affichera d'abord deux heatmap concernant avec comme donnée le nombre de trajet entre deux stations, puis la même chose mais en fonction de l'heure de la journée avec un slider.


*   Pour `build_station_heatmap` qui correspond à notre première heatmap : On récupère d'abord le dataframe avec les paires de nos stations. Ensuite, on compte le nombre de paires pour chaque paires, on va ensuite décomposer les paires en deux colonnes (station de départ | station d'arrivée). On créé ensuite la heatmap.
*   Pour `build_interactive_heatmap` qui correspond à notre seconde heatmap : le principe est similaire, seulement on récupère aussi l'heure de la journée `hour_of_day`, que l'on réutilise pour le slider.
"""

import plotly.graph_objects as go
from pyspark.sql.functions import concat_ws

# On choisi l'année 2019
df_2019 = {key: df for key, df in dataframescopy.items() if '2019' in key}

# Création du df correspondant à l'année 2019 : union de chaque dataframe "mois"
df_union_2019 = None
for mois, df in df_2019.items():
  if df_union_2019 is None:
    df_union_2019 = df
  else :
    df_union_2019 = df_union_2019.union(df)


# On travaille sur le mois de mars 2019, pour que le programme prennent moins de temps à compiler.
df = dataframescopy["201903"]

def build_station_heatmap(df, title):
    # On concatène les noms des stations de départ et d'arrivée
    df = df.withColumn("station_pair", concat_ws(" --> ", "start_station_name", "end_station_name"))

    # On compte le nombre de trajets pour chaque paire de stations
    station_pairs = df.groupBy("station_pair").count().toPandas()

    # On décompose les paires en deux colonnes séparées pour les axes
    station_pairs[['start_station', 'end_station']] = station_pairs['station_pair'].str.split(' --> ', expand=True)

    # Pivot table pour créer une matrice pour la heatmap
    pivot_table = station_pairs.pivot(index='start_station', columns='end_station', values='count').fillna(0)

    fig = go.Figure(data=go.Heatmap(
        z=pivot_table.values,
        x=pivot_table.columns,
        y=pivot_table.index,
        colorscale='Viridis'
    ))

    fig.update_layout(
        title=title,
        xaxis_nticks=36,
        yaxis_nticks=36
    )

    fig.show()

build_station_heatmap(df, "Heatmap des trajets entre stations en Mars 2019")




def build_interactive_heatmap(df, title):
    # On récupère l'heure de départ cette fois-ci
    df = df.withColumn("hour_of_day", hour("started_at"))

    # On concatène les noms des stations de départ et d'arrivée
    df = df.withColumn("station_pair", concat_ws(" --> ", "start_station_name", "end_station_name"))

    # On compte le nombre de trajets pour chaque paire de stations et heure de la journée
    station_pairs_by_hour = df.groupBy("station_pair", "hour_of_day").count().toPandas()

    # On décompose les paires en deux colonnes séparées pour les axes
    station_pairs_by_hour[['start_station', 'end_station']] = station_pairs_by_hour['station_pair'].str.split(' --> ', expand=True)

    fig = px.density_heatmap(
        station_pairs_by_hour,
        x='start_station',
        y='end_station',
        z='count',
        animation_frame='hour_of_day',
        color_continuous_scale='Viridis',
        title=title
    )

    fig.update_layout(
        xaxis_nticks=36,
        yaxis_nticks=36
    )

    fig.show()

build_interactive_heatmap(df, "Heatmap interactive des trajets entre stations en Mars 2019")

# Affichage pour le dataframe de l'année 2019 : beaucoup plus long, vous pouvez passer

build_station_heatmap(df_union_2019, "Heatmap des trajets entre stations en 2019")
build_interactive_heatmap(df_union_2019, "Heatmap interactive des trajets entre stations en 2019")

"""### Choropleth Map

Ensuite, pour la suite de cette partie, on utilisera plotly pour afficher une première map qui correspondra a afficher sur une carte, centré sur New York, les stations ainsi que le nombre de fois où elles ont été une station de départ.
On aura une seconde carte qui utilisera ipyleaflet et qui affichera les stations par durée moyenne de trajet.



*   `start_station_counts_choropleth_map` correspond à notre première map. On utilise un geoDataFrame de geopandas. On défini les limites de la carte, par rapport aux coordonnées géographique de New York. On affichera par un dégradé de couleurs les différentes variation en fonction du nombres de départ.

*   Pour `avg_trip_duration_map`, qui correspond à notre seconde map, on utilise donc ipyleaflet. On créé une fonction auxiliaire `get_color` pour afficher des meilleurs marqueurs en couleur sur la map.



"""

import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
from ipyleaflet import Map, Marker, CircleMarker, Popup




def start_station_counts_choropleth_map(df,title):
  df = df.toPandas()

  # On crée un GeoDataFrame à partir du DataFrame Pandas
  gdf = gpd.GeoDataFrame(
      df,
      geometry=gpd.points_from_xy(df.start_lng, df.start_lat)
  )

  # On défini les limites de la carte pour afficher New York plus largement
  map_center = {"lat": 40.7128, "lon": -74.0060}  #Coordonnées de New York
  map_zoom = 11

  # Carte en plotly
  fig = px.scatter_mapbox(
      gdf,
      lat=gdf.geometry.y,
      lon=gdf.geometry.x,
      color="ride_count",
      hover_name="start_station_name",
      hover_data=["ride_count"],
      center=map_center,
      zoom=map_zoom,
      mapbox_style="carto-positron",
      title=title
  )

  # Afficher la carte
  fig.show()


start_station_counts_03_2019 = df.groupBy("start_station_name", "start_lat", "start_lng").agg(count("*").alias("ride_count"))
start_station_counts_choropleth_map(start_station_counts_03_2019,"Number of Rides per Start Station in March 2019")
start_station_counts_2019 = df_union_2019.groupBy("start_station_name", "start_lat", "start_lng").agg(count("*").alias("ride_count"))
start_station_counts_choropleth_map(start_station_counts_2019,"Number of Rides per Start Station in 2019")

from ipywidgets import HTML
import matplotlib.colors as mcolors

# Fonction auxiliaire pour définir la palette de couleur utilisé pour les marqueurs
def get_color(duration, min_duration, max_duration):
    # On normalise la durée pour qu'elle soit entre 0 et 1
    norm = mcolors.Normalize(vmin=min_duration, vmax=max_duration)
    # Création un mappage de couleur
    cmap = mcolors.LinearSegmentedColormap.from_list("duration_cmap", ["green", "yellow", "red"])
    return mcolors.to_hex(cmap(norm(duration)))



def avg_trip_duration_map(df):
    df = df.toPandas()
    # Créer un GeoDataFrame à partir du DataFrame Pandas
    gdf = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.start_lng, df.start_lat)
    )

    # Pour "calculer" les couleurs des marqueurs, on cherche le min et max du temps moyen.
    min_duration = gdf['avg_trip_duration'].min()
    max_duration = gdf['avg_trip_duration'].max()

    # On crée la carte centrée sur New York
    center = (40.7128, -74.0060)
    m = Map(center=center, zoom=11)

    # On ajoute les marqueurs pour chaque station :
    for idx, row in gdf.iterrows():
        # On récupère la couleur du marqueur
        color = get_color(row['avg_trip_duration'], min_duration, max_duration)

        # On crée un marqueur
        marker = CircleMarker(
            location=(row['start_lat'], row['start_lng']),
            radius=5,  # Taille du marqueur
            color=color,
            fill_color=color,
            fill_opacity=0.6,
        )

        # On crée enfin un popup avec le nom de la station et la durée moyenne des trajets
        html = HTML()
        html.value = f"<b>{row['start_station_name']}</b>: {row['avg_trip_duration']:.2f} minutes"

        popup = Popup(
            location=(row['start_lat'], row['start_lng']),
            child=html,
            close_button=False,
            auto_close=False,
            close_on_escape_key=False
        )

        # On fait la liaison entre le popup et le marqueur
        marker.popup = popup

        # Ajout du marqueur sur la carte
        m.add_layer(marker)

    # Affiche la carte
    display(m)

  # Calcule la durée moyenne  par trajet sur l'année 2019
avg_trip_duration_2019 = df_union_2019.groupBy("start_station_name", "start_lat", "start_lng").agg(avg("trip_duration").alias("avg_trip_duration"))
avg_trip_duration_map(avg_trip_duration_2019)